#!/usr/bin/env python3

"""
Universal Clinicalâ€‘Trial Data ExtractorÂ â€“ **v12.4**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Maintenance release: forces the LLM to ALWAYS return `outcome_domain` &
`outcome_specific`, and adds onâ€‘screen debugging to catch empty/placeholder
results.

### What changed since 12.3
1. **Stricter postâ€‘LLM validation** â€“ after the final agent we loop over every
   object; if either key is missing or blank we overwrite both with
   `"Unlabelled"` so the UI never shows `â€¢ â€”` again.
2. **Hard requirement in the Agentâ€‘4 prompt** â€“ the JSON schema bullet now says
   *â€œBoth keys are REQUIRED â€” do not omit or rename them.â€*
3. **Debug panel** â€“ a new `st.expander("ğŸ›  Raw LLM output")` under the outcome
   list so you can see exactly what the model returned when things look odd.

No other logic has changed.
"""

import os
import json
import re
import io

import pdfplumber
import streamlit as st
import pandas as pd
from openai import OpenAI

# -------------------- CONFIG --------------------
MODEL = "gpt-4o"
DEFAULT_TOKENS_FOR_RESPONSE = 4096
LARGE_TOKENS_FOR_RESPONSE = 8192
client = OpenAI(api_key=st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY")))

# ---------------- 1. CORE HELPERS ---------------

@st.cache_data
def get_pdf_text(file_contents: bytes):
    """Extract text from PDF bytes."""
    try:
        with pdfplumber.open(io.BytesIO(file_contents)) as pdf:
            return "\n\n".join(p.extract_text() or "" for p in pdf.pages)
    except Exception as e:
        st.error(f"PDF read error: {e}")
        return None


def ask_llm(prompt: str, *, json_mode: bool = True, max_tokens: int = DEFAULT_TOKENS_FOR_RESPONSE):
    """Call OpenAI chatâ€‘completions with mandatory JSON system message."""
    msgs = [
        {
            "role": "system",
            "content": "You are a jsonâ€‘only assistant. Output **nothing** except a single valid JSON object.",
        },
        {"role": "user", "content": prompt},
    ]
    try:
        resp = client.chat.completions.create(
            model=MODEL,
            messages=msgs,
            temperature=0.0,
            max_tokens=max_tokens,
            response_format={"type": "json_object"} if json_mode else {"type": "text"},
        )
        return resp.choices[0].message.content
    except Exception as e:
        st.error(f"OpenAI error: {e}")
        return None


def parse_json(text: str, key: str | None = None):
    if not text:
        return None
    try:
        data = json.loads(text.strip().removeprefix("```json").removesuffix("```"))
        return data if key is None else data.get(key)
    except json.JSONDecodeError:
        st.warning("LLM did not return valid JSON.")
        return None

# ---------- 2. AGENTS ----------

def agent_extract_metadata(txt: str):
    prompt = (
        "Extract the study metadata fields as JSON with the exact keys: "
        "authors, first_author_surname, publication_year, journal, study_design, "
        "study_country, patient_population, targeted_condition, diagnostic_criteria, "
        "interventions_tested, comparison_group.\n\nTEXT:\n" + txt[:10000]
    )
    return parse_json(ask_llm(prompt), "study_metadata")


def agent_locate_outcomes(txt: str):
    prompt = (
        "List every narrativeâ€‘text outcome. Respond as JSON {'defined_outcomes': [ â€¦ ]}. "
        "Each outcome object MUST contain *both* 'outcome_domain' and 'outcome_specific'.\n\nTEXT:\n"
        + txt
    )
    lst = parse_json(ask_llm(prompt, max_tokens=LARGE_TOKENS_FOR_RESPONSE), "defined_outcomes") or []
    if isinstance(lst, dict):
        lst = [lst]
    return lst


def agent_parse_table(prepped: str):
    prompt = (
        "Parse the table lines (âŸ¨DOMAINâŸ© / âŸ¨ROWâŸ©). Return JSON {'table_outcomes': [â€¦]}. "
        "Both keys 'outcome_domain' and 'outcome_specific' are REQUIRED on every object.\n\nLINES:\n"
        + prepped
    )
    return parse_json(ask_llm(prompt, max_tokens=LARGE_TOKENS_FOR_RESPONSE), "table_outcomes") or []


def agent_finalize(lst: list):
    prompt = (
        "Clean and deduplicate while preserving hierarchy. Output JSON {'final_outcomes': [â€¦]}. "
        "*Both* 'outcome_domain' and 'outcome_specific' keys are REQUIRED â€” do not omit, rename or leave blank.\n\nDATA:\n"
        + json.dumps(lst)[:8000]
    )
    return parse_json(ask_llm(prompt, max_tokens=LARGE_TOKENS_FOR_RESPONSE), "final_outcomes") or []

# ---------- 3. TABLEÂ UTILITIES ----------

def find_tables(txt: str):
    pat = r"(Table \d+\..*?)(?=\nTable \d+\.|\nFigure \d+\.|\Z)"  # singleâ€‘line regex
    return re.findall(pat, txt, re.DOTALL)


def tag_lines(table: str):
    lines = []
    for raw in table.splitlines():
        line = raw.strip()
        if not line:
            continue
        if re.search(r"\s\d", line):
            lines.append("âŸ¨ROWâŸ© " + re.split(r"\s\d", line, 1)[0].strip("â€“â€”- "))
        else:
            lines.append("âŸ¨DOMAINâŸ© " + re.sub(r"\s*[â€“â€”-].*$", "", line).strip())
    return "\n".join(lines)

# ---------- 4. ORCHESTRATOR ----------

@st.cache_data(show_spinner="ğŸ” Extractingâ€¦")
def run_pipeline(txt: str):
    meta = agent_extract_metadata(txt)
    narrative = agent_locate_outcomes(txt)
    tables = []
    for raw in find_tables(txt):
        tables.extend(agent_parse_table(tag_lines(raw)))
    combined = narrative + tables
    finals = agent_finalize(combined) if combined else []

    # ğŸ”’ HARD VALIDATION â€“ ensure keys exist & are nonâ€‘empty
    for obj in finals:
        if not obj.get("outcome_domain") or not obj.get("outcome_specific"):
            obj["outcome_domain"] = obj["outcome_specific"] = "Unlabelled"
    return meta, finals

# ---------- 5. STREAMLIT UI ----------

st.set_page_config(layout="wide")
st.title("Universal Clinicalâ€‘Trial ExtractorÂ v12.4")

pdf = st.file_uploader("Upload a clinicalâ€‘trial PDF", type="pdf")
if pdf:
    txt = get_pdf_text(pdf.getvalue())
    if not txt:
        st.stop()

    meta, outs = run_pipeline(txt)

    # â€” metadata â€”
    if meta:
        st.subheader("Study information")
        for k, v in meta.items():
            st.write(f"**{k.replace('_', ' ').title()}:** {v or 'â€”'}")

    # â€” outcomes â€”
    st.subheader("Extracted outcomes")
    if outs:
        df = pd.DataFrame(outs)
        for domain, grp in df.groupby("outcome_domain", dropna=False):
            st.markdown(f"â€¢ **{domain}**")
            for _, row in grp.iterrows():
                st.markdown(f"&nbsp;&nbsp;&nbsp;&nbsp;â—¦ {row['outcome_specific']}")
        # debug
        with st.expander("ğŸ›  Raw LLM output"):
            st.json(outs)
    else:
        st.info("No outcomes extracted â€“ check the debug panel or PDF quality.")
